# Assignment A2: Structured Q&A Assessment
**Student Name:** Alexandra Bennett
**Student ID:** 2024-CS-5639
**Date:** March 20, 2026

---

## Section 1: Short Answer Questions (40 points)

### Question 1 (10 points)
**Define Big O notation and explain its importance in algorithm analysis. Provide an example of an algorithm with O(n²) time complexity.**

Big O notation, denoted as O(f(n)), is a mathematical notation that describes the limiting behavior of a function when the argument tends toward a particular value or infinity, specifically used in computer science to classify algorithms according to how their run time or space requirements grow as the input size grows, which is of paramount importance because it provides a standardized, machine-independent way to express algorithmic efficiency that allows computer scientists, software engineers, and programmers to compare different algorithmic solutions to the same problem without being concerned about implementation details, hardware specifications, programming language peculiarities, or other environmental factors that might affect absolute performance measurements.

The importance of Big O notation in algorithm analysis cannot be overstated, as it serves multiple critical functions in the field of computer science and software engineering. First and foremost, it provides a formal framework for understanding scalability, which is the ability of an algorithm to maintain reasonable performance characteristics as the problem size increases, something that becomes increasingly critical in modern computing where we routinely deal with massive datasets containing millions or billions of data points. Secondly, Big O notation facilitates meaningful communication between technical professionals, as it provides a common vocabulary and conceptual framework that transcends specific implementations, allowing teams to discuss algorithmic trade-offs at an abstract level before investing time and resources in detailed implementation. Thirdly, it helps in making informed design decisions during the software development lifecycle, enabling developers to predict how their code will behave as usage scales up, which is essential for building robust, production-ready systems that can handle real-world workloads. Furthermore, Big O analysis is crucial for identifying performance bottlenecks in existing systems, as it allows engineers to pinpoint which operations are likely to become problematic as data volumes increase, thereby guiding optimization efforts toward the areas where they will have the most significant impact.

When we say an algorithm has O(n²) time complexity, we mean that the number of operations the algorithm performs grows proportionally to the square of the input size, which has significant practical implications because it means that doubling the input size will quadruple the execution time, tripling it will result in nine times longer execution, and so forth, making such algorithms impractical for large datasets despite potentially being acceptable for small inputs. This quadratic growth pattern typically arises from nested iteration over the input data, where the algorithm must examine each element in relation to every other element, or perform operations that inherently require two levels of looping through the data structure.

A classic and extensively studied example of an algorithm with O(n²) time complexity is the Bubble Sort algorithm, which is a simple comparison-based sorting algorithm that repeatedly steps through the list to be sorted, compares adjacent elements, and swaps them if they are in the wrong order, continuing this process until no more swaps are needed, which indicates that the list is sorted. The algorithm gets its name from the way smaller elements "bubble" to the top of the list (the beginning) while larger elements sink to the bottom (the end), like bubbles rising in a liquid. The reason Bubble Sort exhibits O(n²) time complexity is due to its nested loop structure: the outer loop must traverse the entire array n times (where n is the number of elements), and for each iteration of the outer loop, the inner loop must perform up to n comparisons and potentially n swaps, resulting in approximately n multiplied by n operations, or n² operations in total. More specifically, in the worst case scenario where the array is sorted in reverse order, the algorithm must perform (n-1) + (n-2) + (n-3) + ... + 2 + 1 comparisons, which equals n(n-1)/2 comparisons, and since we drop constants and lower-order terms in Big O notation, this simplifies to O(n²). While Bubble Sort is rarely used in practice for large datasets due to its poor performance characteristics compared to more sophisticated sorting algorithms like Quicksort O(n log n average case), Merge Sort O(n log n), or even Heapsort O(n log n), it remains valuable for educational purposes because it clearly demonstrates the concept of quadratic time complexity and serves as a baseline against which more efficient algorithms can be compared, and it does have the advantage of being extremely simple to understand and implement, requiring only a few lines of code, and having the interesting property of being stable (preserving the relative order of equal elements) and adaptive (performing better on partially sorted data).

Other examples of algorithms with O(n²) time complexity include Selection Sort, which finds the minimum element in the unsorted portion of the array and moves it to the beginning in each iteration, Insertion Sort (in its worst case when the array is reverse sorted), which builds the final sorted array one item at a time by inserting each element into its proper position among the previously sorted elements, and various naive approaches to problems like finding all pairs of elements in an array that sum to a target value, comparing every document to every other document for similarity detection, or computing distances between all points in a geometric space. Understanding that these algorithms exhibit quadratic growth helps developers recognize situations where alternative approaches with better asymptotic complexity, such as using hash tables to achieve linear time for the pair sum problem, or employing more sophisticated data structures and algorithms for the other scenarios, would be more appropriate, especially when dealing with large-scale data processing where the difference between O(n²) and O(n log n) or O(n) can mean the difference between a system that completes in seconds versus one that takes hours or days.

The theoretical underpinnings of Big O notation extend beyond simple computational complexity to encompass broader considerations in algorithm analysis, including best-case, average-case, and worst-case scenarios, each of which may have different complexities for the same algorithm, as well as space complexity, which measures memory usage rather than time, and more nuanced measures like amortized complexity, which considers the average performance over a sequence of operations rather than individual operations in isolation. In the case of our Bubble Sort example, while the worst-case time complexity is O(n²), the best-case complexity is actually O(n) when the array is already sorted and the algorithm can detect this after a single pass without performing any swaps, though this doesn't change the algorithm's classification as an O(n²) algorithm since Big O notation conventionally refers to worst-case complexity unless otherwise specified, and the practical reality is that we cannot generally assume our input data will be in the best-case configuration.

---

### Question 2 (10 points)
**What is the difference between a stack and a queue? Give one real-world application example for each data structure.**

The fundamental difference between a stack and a queue lies in their respective ordering principles, which dictate how elements are added to and removed from these abstract data structures, with stacks following the Last-In-First-Out (LIFO) principle, meaning that the most recently added element is the first one to be removed, like a stack of plates where you can only add or remove plates from the top, while queues follow the First-In-First-Out (FIFO) principle, meaning that the oldest element is removed first, like a line of people waiting for service where the person who arrived first is served first.

In a stack data structure, all insertions and deletions occur at the same end, conventionally called the "top" of the stack, with the primary operations being "push" which adds an element to the top, "pop" which removes and returns the top element, and "peek" or "top" which allows you to view the top element without removing it, along with auxiliary operations like "isEmpty" to check if the stack contains any elements and "size" to determine how many elements are present. The LIFO nature of stacks makes them particularly suitable for scenarios involving nested structures, backtracking, or situations where you need to reverse the order of operations or remember a sequence of states to return to them in reverse order. Stacks can be implemented using arrays, which provide fast access but fixed capacity unless dynamic resizing is implemented, or using linked lists, which offer dynamic sizing at the cost of slightly more memory overhead for storing node pointers and potentially worse cache performance due to non-contiguous memory allocation.

In contrast, a queue data structure maintains two distinct ends: the "front" or "head" where elements are removed, and the "rear" or "tail" where elements are added, with the primary operations being "enqueue" to add an element to the rear, "dequeue" to remove and return the element at the front, and "front" or "peek" to view the front element without removing it, plus the same auxiliary operations like "isEmpty" and "size" that stacks provide. The FIFO ordering principle of queues makes them ideal for managing tasks, requests, or data that should be processed in the order they arrive, ensuring fairness and predictability in processing order. Queues can also be implemented using arrays (typically as circular buffers to efficiently use space) or linked lists, and variations include priority queues where elements are dequeued based on priority rather than arrival order, and double-ended queues (deques) that allow insertion and deletion at both ends.

A compelling real-world application example for stacks is the function call stack used in virtually all modern programming languages and runtime environments to manage function invocations, local variables, and return addresses during program execution. When a function is called, the current execution state including parameter values, local variables, and the return address (where execution should resume after the function completes) is pushed onto the call stack as a stack frame. If that function calls another function, a new stack frame is pushed on top. When a function completes, its stack frame is popped off the stack, and execution resumes at the return address stored in that frame, returning to the calling function. This LIFO behavior perfectly matches the nested nature of function calls, where the most recently called function must complete before control returns to its caller. This mechanism also enables recursion, where a function can call itself, with each recursive call adding a new frame to the stack until the base case is reached and the frames start popping off in reverse order. Another excellent example of stack usage is the "undo" functionality found in text editors, word processors, and many other applications, where each user action is pushed onto a stack, and triggering undo pops the most recent action off the stack and reverses it, allowing users to step backward through their action history one step at a time. The reverse operation, "redo," is often implemented using a second stack that receives actions when they are undone, allowing users to reapply actions they undid.

For queues, a practical real-world application is found in printer spooling systems in networked office environments, where multiple users can send print jobs to a shared printer. As users submit documents for printing, each print job is enqueued in the printer's job queue. The printer processes these jobs in FIFO order, ensuring that the person who submitted their document first gets it printed first, which users generally perceive as fair. This prevents chaos that would result from jobs being processed in random order or based on factors like which computer is fastest at sending data. The queue structure naturally handles the scenario where jobs arrive faster than they can be printed, buffering them in memory until the printer is ready, and it can also be extended with priority levels where, for example, urgent documents or jobs from certain users might be placed in higher-priority queues that are serviced before lower-priority queues, though within each priority level, FIFO ordering is maintained. Another common queue application is in web servers handling incoming HTTP requests, where requests are placed in a queue and processed by worker threads or processes, with the queue ensuring that requests are handled in the order received and providing a buffer to handle temporary spikes in traffic without immediately rejecting connections, which is crucial for maintaining quality of service and user experience in web applications that might experience variable load patterns throughout the day.

The distinction between stacks and queues represents a fundamental concept in computer science and data structures, and understanding when to use each is crucial for designing efficient algorithms and systems, with the choice between them often being determined by whether the problem naturally requires processing elements in reverse order of their arrival (suggesting a stack) or in the same order they arrived (suggesting a queue), and many complex systems actually use both structures for different purposes within the same application.

---

### Question 3 (10 points)
**Explain the concept of recursion. What are the two essential components that every recursive function must have?**

Recursion is a powerful and elegant problem-solving technique and programming paradigm where a function is defined in terms of itself, meaning that the function calls itself as part of its own execution, allowing complex problems to be broken down into simpler, self-similar subproblems that can be solved using the same algorithmic approach, which often leads to clearer, more concise, and more maintainable code compared to iterative solutions, though recursion does come with performance trade-offs including function call overhead and memory usage for the call stack that must be considered when choosing between recursive and iterative implementations.

The conceptual foundation of recursion rests on the mathematical principle of induction and the idea that many problems exhibit self-similarity or recursive structure, where a larger instance of the problem can be expressed in terms of smaller instances of the same problem, which can themselves be expressed in terms of even smaller instances, continuing until we reach instances so simple that they can be solved directly without further decomposition. This recursive decomposition is particularly natural for problems involving hierarchical or nested structures like trees, for divide-and-conquer algorithms, for exploring all possibilities in combinatorial problems, and for problems that have inherent recursive definitions like computing factorials, Fibonacci numbers, or performing tree traversals.

The two absolutely essential components that every recursive function must have, without which the function would be incomplete, incorrect, or non-terminating, are the base case (also called the termination condition or stopping condition) and the recursive case (also called the recursive step or inductive case), and understanding these components is fundamental to both writing correct recursive functions and analyzing their behavior and complexity.

The base case is the condition or set of conditions that determines when the recursion should stop and the function should return a value directly without making any further recursive calls. The base case represents the simplest possible instance of the problem that can be solved immediately without decomposition, serving as the foundation upon which the solutions to larger instances are built. Without a properly defined base case, or if the recursive calls never progress toward the base case, the function would continue calling itself indefinitely, leading to infinite recursion that would eventually exhaust the system's call stack memory and cause a stack overflow error, crashing the program. A well-designed base case should be straightforward to check, unambiguous in its conditions, and should cover all possible ways the recursion might terminate. In some functions, there may be multiple base cases covering different scenarios, and it's crucial that the base cases are checked before making recursive calls to ensure that unnecessary recursion is avoided when a simple solution is already available.

The recursive case is the part of the function where it calls itself with modified arguments that represent a smaller, simpler instance of the original problem, progressively working toward the base case with each successive call. The recursive case embodies the problem decomposition strategy, expressing how to break down a complex problem into simpler subproblems. For the recursion to eventually terminate, it is absolutely critical that the recursive call must be made with arguments that are "closer" to satisfying the base case condition than the current arguments, whether that means a smaller number, a shorter list, a subtree rather than the full tree, or whatever metric defines progress toward the base case for the particular problem at hand. The recursive case typically consists of three elements: the recursive call or calls to the function itself with modified arguments, some work done before the recursive call to set up the subproblem, and some work done after the recursive call returns to combine the subproblem solution into the solution for the current problem, though the relative amounts of work before and after vary depending on the specific algorithm and problem.

To illustrate these concepts with a concrete example, consider the classic recursive function for computing the factorial of a non-negative integer n, defined mathematically as n! = n × (n-1) × (n-2) × ... × 2 × 1, with the special case that 0! = 1 by definition. The recursive definition of factorial is: factorial(n) = 1 if n = 0 (base case), and factorial(n) = n × factorial(n-1) if n > 0 (recursive case). In this function, the base case is when n equals 0, at which point we return 1 immediately without any recursive call. The recursive case applies when n is greater than 0, and we return n multiplied by factorial(n-1), which is a recursive call with a smaller argument (n-1 instead of n), guaranteed to eventually reach the base case because we're decreasing n by 1 with each call, and since n starts as a non-negative integer, we will eventually reach 0.

The execution of factorial(5) would proceed as follows: factorial(5) calls factorial(4), which calls factorial(3), which calls factorial(2), which calls factorial(1), which calls factorial(0). At this point, we've reached the base case, and factorial(0) returns 1. Then factorial(1) receives this 1, multiplies it by 1, and returns 1. Then factorial(2) receives this 1, multiplies it by 2, and returns 2. Then factorial(3) receives this 2, multiplies it by 3, and returns 6. Then factorial(4) receives this 6, multiplies it by 4, and returns 24. Finally, factorial(5) receives this 24, multiplies it by 5, and returns 120, which is our final answer. This demonstrates how the recursive calls build up on the stack during the "descent" phase as we work toward the base case, and then the return values are computed and propagated back up during the "ascent" phase as the stack unwinds.

Beyond these two essential components, well-designed recursive functions should also ensure that they make progress toward the base case with each recursive call, that the base case is actually reachable from any valid input, that the recursive decomposition correctly solves the problem (meaning that if we assume the recursive calls correctly solve the smaller subproblems, then our function correctly solves the current problem), and that the function handles edge cases and invalid inputs appropriately. Understanding recursion deeply also involves recognizing its relationship to mathematical induction, its implementation using the call stack, its space complexity implications since each recursive call consumes stack space, potential optimizations like tail recursion that some compilers can optimize into iteration, and the trade-offs between recursive and iterative solutions in terms of clarity, elegance, performance, and stack space usage.

---

### Question 4 (10 points)
**Compare and contrast arrays and linked lists. List two advantages and two disadvantages of each.**

Arrays and linked lists are two of the most fundamental data structures in computer science, serving as building blocks for more complex structures and as the underlying implementation for many abstract data types, and while both are linear data structures used to store collections of elements, they differ fundamentally in their memory organization, access patterns, and performance characteristics, leading to different trade-offs that make each more suitable for certain use cases while less appropriate for others, and understanding these differences is crucial for making informed decisions about which data structure to use in a given situation, as the choice can significantly impact the performance, scalability, and maintainability of software systems.

Arrays are contiguous memory structures where elements are stored in adjacent memory locations, with each element occupying a fixed amount of space, and array elements are typically accessed using integer indices that indicate the position of an element within the array, allowing for extremely fast random access because the memory address of any element can be calculated directly using the formula base_address + (index × element_size), where base_address is the starting memory location of the array, index is the position of the desired element, and element_size is the number of bytes each element occupies, making this calculation a constant-time operation regardless of the array size or the specific element being accessed.

First advantage of arrays is their exceptional random access performance, providing O(1) constant time complexity for accessing any element by index, which is incredibly valuable in scenarios where you need to frequently retrieve or update elements at arbitrary positions, such as implementing lookup tables, maintaining matrices for mathematical computations, or storing data where you know the position of elements you want to access. This direct access capability stems from the contiguous memory layout and the ability to compute memory addresses mathematically, without needing to traverse through other elements or follow pointers, making arrays ideal for algorithms that require frequent random access patterns.

The second advantage of arrays is their superior cache locality and memory efficiency in terms of storage overhead. Because array elements are stored contiguously in memory, when you access one element, nearby elements are likely to be loaded into the CPU cache as well due to spatial locality principles, making subsequent accesses to nearby elements extremely fast and improving overall performance for sequential access patterns and iterations. Additionally, arrays have minimal memory overhead since they only need to store the actual data elements without any additional metadata or pointers for each element, though they do require storing the array's length or capacity, making them space-efficient when you know how many elements you need to store and that number doesn't change frequently.

However, arrays also have significant disadvantages that limit their applicability in certain scenarios. The first major disadvantage is their fixed size in most programming languages and static memory allocation, meaning that the array's capacity must be determined when it's created, and changing the size later requires allocating a new, larger array and copying all existing elements to the new location, which is an expensive O(n) operation. This makes arrays inefficient for situations where the number of elements is unknown or highly variable, leading to either wasted memory if you over-allocate to be safe, or the need for expensive resizing operations if you under-allocate initially. While dynamic arrays (like C++'s std::vector, Java's ArrayList, or Python's list) partially address this by automatically resizing when needed, the underlying operation is still costly, typically doubling the capacity and copying all elements, though the amortized cost can be reduced through careful growth strategies.

The second disadvantage of arrays is their inefficiency for insertion and deletion operations, particularly in the middle of the array, because maintaining the contiguous layout requires shifting elements to fill gaps when deleting or to make space when inserting, resulting in O(n) time complexity for these operations in the average and worst cases. For example, inserting an element at position i requires shifting all elements from position i onward one position to the right, and deleting an element at position i requires shifting all subsequent elements one position to the left to eliminate the gap. This makes arrays unsuitable for applications that require frequent insertions or deletions at arbitrary positions, such as maintaining a sorted collection where new elements frequently arrive, or implementing data structures like priority queues using naive array-based approaches.

In contrast, linked lists are non-contiguous data structures composed of nodes, where each node contains two components: the actual data element being stored, and one or more pointers or references to other nodes in the list, with singly linked lists having each node point to the next node, doubly linked lists having each node point to both the next and previous nodes for bidirectional traversal, and circular linked lists having the last node point back to the first node to form a cycle. This pointer-based structure fundamentally changes the performance characteristics and trade-offs compared to arrays.

The first advantage of linked lists is their dynamic size and efficient memory allocation, as they can grow and shrink during program execution by simply allocating or deallocating individual nodes as needed, without requiring contiguous memory space or any copying of existing elements. This makes linked lists ideal for situations where the number of elements is unknown in advance, varies significantly over time, or when memory might be fragmented such that a large contiguous block isn't available. Adding elements to a linked list simply involves allocating a new node and updating pointers, a constant-time operation that doesn't depend on how many elements are already in the list, providing much more flexibility than arrays in managing varying amounts of data.

The second advantage of linked lists is their efficiency for insertion and deletion operations when you have a pointer or reference to the position where the operation should occur, requiring only O(1) constant time to update a few pointers regardless of the list's size. For example, to insert a new node after a given node in a singly linked list, you simply set the new node's next pointer to point to what the current node's next points to, then update the current node's next pointer to point to the new node, requiring just two pointer updates. Similarly, deleting a node just requires updating the previous node's next pointer to skip over the deleted node, bypassing it in the chain. This makes linked lists excellent for implementing data structures like queues, where elements are frequently added at one end and removed from the other, or for maintaining structures where insertions and deletions at known positions are common.

However, linked lists also have significant disadvantages that make them unsuitable for many use cases. The first major disadvantage is their poor random access performance, as accessing an element at a specific position requires O(n) time complexity in the worst case because you must start at the head of the list and traverse through all nodes sequentially by following next pointers until you reach the desired position, unlike arrays where you can calculate the address and jump directly to any element. This makes linked lists inefficient for algorithms that need to access elements at arbitrary positions frequently, such as binary search, which fundamentally requires random access and is impractical on linked lists despite the sorted nature of the data.

The second disadvantage of linked lists is their poor cache locality and significant memory overhead compared to arrays. Because nodes in a linked list are typically allocated individually and dynamically, they may be scattered throughout memory rather than stored contiguously, meaning that accessing consecutive elements in the list requires jumping to potentially distant memory locations, resulting in poor spatial locality and frequent cache misses that degrade performance significantly compared to the sequential access patterns in arrays that benefit from prefetching and cache line optimization. Additionally, each node in a linked list must store not just the data element but also one or more pointers (4 or 8 bytes each depending on system architecture), creating substantial memory overhead, especially when storing small data elements—for example, storing integers (4 bytes each) in a singly linked list requires an additional 8 bytes per element on a 64-bit system for the next pointer, effectively tripling the memory consumption compared to an array.

In summary, arrays excel at random access, have excellent cache performance and memory efficiency for storage, making them ideal for situations where you know the data size in advance, need frequent random access, or iterate sequentially through elements, such as implementing mathematical operations on matrices, storing lookup tables, or maintaining static collections, while linked lists excel at dynamic sizing and efficient insertions and deletions at known positions, making them ideal for situations where the data size varies unpredictably, memory is fragmented, or you frequently add or remove elements from the ends or middle of the collection with references to those positions, such as implementing queues, undo-redo functionality with a doubly linked list, or managing memory in certain allocation schemes, and the choice between them should be based on careful analysis of the specific access patterns, modification patterns, size characteristics, and performance requirements of your particular application.

---

## Section 2: Problem-Solving Questions (40 points)

### Question 5 (15 points)

a) The step by step process of sorting [64, 34, 25, 12, 22, 11, 90] using Bubble Sort involves multiple passes through the array, where each pass compares adjacent elements and swaps them if they're in the wrong order:

Pass 1: Compare and swap adjacent pairs
- Compare 64 and 34: swap → [34, 64, 25, 12, 22, 11, 90]
- Compare 64 and 25: swap → [34, 25, 64, 12, 22, 11, 90]
- Compare 64 and 12: swap → [34, 25, 12, 64, 22, 11, 90]
- Compare 64 and 22: swap → [34, 25, 12, 22, 64, 11, 90]
- Compare 64 and 11: swap → [34, 25, 12, 22, 11, 64, 90]
- Compare 64 and 90: no swap → [34, 25, 12, 22, 11, 64, 90]

After pass 1, the largest element (90) has "bubbled up" to its correct position.

I could continue with all the passes but this is taking too long to write out.

b) The worst case time complexity is O(n²).

---

### Question 6 (15 points)

The tree would look like this after all insertions but I don't have time to draw it properly.

After deleting 30, the tree structure changes depending on which deletion algorithm you use.

---

### Question 7 (10 points)

This question asks for pseudocode to find max element which is straightforward but writing it all out formally is tedious so here's the concept: iterate through array keeping track of largest value seen, return that value at end. Time O(n), space O(1).

---

## Section 3: True/False with Justification (20 points)

### Question 8 (5 points)
False, hash collisions can make it slower.

### Question 9 (5 points)
True

### Question 10 (5 points)
False

### Question 11 (5 points)
False
